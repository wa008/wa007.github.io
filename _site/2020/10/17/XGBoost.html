<p>«««&lt; HEAD</p>
<h2 id="xgboos博客">XGBoos博客</h2>
<p>=======</p>
<h1 id="xgboos博客-1">XGBoos博客</h1>
<blockquote>
  <blockquote>
    <blockquote>
      <blockquote>
        <blockquote>
          <blockquote>
            <blockquote>
              <p>ed15047cc3e39885c43f91e285e01a998ab9ea27</p>
            </blockquote>
          </blockquote>
        </blockquote>
      </blockquote>
    </blockquote>
  </blockquote>
</blockquote>

<p>XGBoost是一种以决策树（cart树）为基学习器的集成学习方法。</p>

<p>XGBoost的目标：
<script type="math/tex">Loss=\large{\sum\nolimits_{i=1}^{n}{l(y_i,\hat{y_i}) + \sum\nolimits_{k=1}^{T}{\Omega(f_k)}}, f_k\in F}</script></p>

<script type="math/tex; mode=display">\large{\Omega(f_k)=\gamma{T}+\frac{1}{2}\lambda\sum\nolimits_{j}^{T}w_j^2}</script>

<p>$\hat{y_i}$代表模型预测值，$\Omega(f_k)$为正则项，代表第k颗树的复杂度</p>

<script type="math/tex; mode=display">% <![CDATA[
\large{\begin{align}
Loss^{(t)} &= \large{\sum\nolimits_{i=1}^{n}{l(y_i,\hat{y_i}^{(t)}) + \sum\nolimits_{k=1}^{T}{\Omega(f_k)}}} \\
  &= \large{\sum\nolimits_{i=1}^{n}{l(y_i,\hat{y_i}^{(t-1)}+f_t({x_i})) + {\Omega(f_t)}}}+const
\end{align}} %]]></script>

<p>泰勒二阶展开为
<script type="math/tex">\large{f(x + \Delta{x}){\approx}f(x)+f^{'}{(x)}{\Delta}x+\frac{1}{2}+f^{''}{(x)}{\Delta}x^2}</script></p>

<p>将泰勒二阶展开应用于Loss函数，可得
<script type="math/tex">\large{Loss{\approx}\sum\nolimits_{i-1}^n{[l(y_i,{\hat{y}}^{t-1})+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]}+{\Omega(f_t)}+const}</script></p>

<script type="math/tex; mode=display">\large{g_i={\partial}_^{t-1}}{l(y_i,{\hat{y}}^{t-1})},h_i={\partial}_^{t-1}}^{2}{l(y_i,{\hat{y}}^{t-1})}}</script>

<p>移除常数项，此次迭代的目标函数为
<script type="math/tex">\large{Object=\sum\nolimits_{i-1}^n{[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]}+{\Omega(f_t)}}</script>
定义$I_j$为第$j$个叶子节点里的样本ID的集合，即$I_j={i|q(x_i)=j}$，将正则项带入
<script type="math/tex">% <![CDATA[
\large{\begin{align}
Object&=\sum\nolimits_{i-1}^n{[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]}+{\Omega(f_t)} \\
&=\sum\nolimits_{i-1}^n{[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]}+\gamma{T}+\frac{1}{2}\lambda\sum\nolimits_{j}^{T}w_j^2 \\
&=\sum_{j-1}^{T}{[(\sum_{j\in{I_j}}g_i)w_i+\frac{1}{2}(\sum_{j\in{I_j}}h_i+\lambda)w_j^2]}+{\lambda}T
\end{align}} %]]></script>
将Loss看作以$f_t{(x_i)}$为自变量的二次函数，求解最小值，即
<script type="math/tex">\large{w_j^*=-\frac{\sum\nolimits_{i{\in}I_j}g_i}{\sum\nolimits_{i{\in}I_j}{h_i}+\lambda}} \\
\large{Object^{(t)}(q)=-\frac{1}{2}{\sum_{j-1}^{T}{\frac{(\sum\nolimits_{i{\in}I_j}g_i)^2}{\sum\nolimits_{i{\in}I_j}h_i+\lambda}}}+{\lambda}T}</script>
假设当前节点的样本集合为$I$,分裂后节点集合为$I_L$和$I_R$，即$I=I_L{ }{\bigcup}{ }I_R$，则此次划分带来的增益为
<script type="math/tex">% <![CDATA[
\large{\begin{align}
L_{split}&=Object_{I_L}+Object_{I_R}-Object_{I} \\
&=\frac{1}{2}{[\frac{(\sum\nolimits_{i{\in}I_L}g_i)^2}{\sum\nolimits_{i{\in}I_L}h_i+\lambda}+\frac{(\sum\nolimits_{i{\in}I_R}g_i)^2}{\sum\nolimits_{i{\in}I_R}h_i+\lambda}-\frac{(\sum\nolimits_{i{\in}I}g_i)^2}{\sum\nolimits_{i{\in}I}h_i+\lambda}]}-\lambda
\end{align}} %]]></script>
以此为增益不断划分节点即可</p>

